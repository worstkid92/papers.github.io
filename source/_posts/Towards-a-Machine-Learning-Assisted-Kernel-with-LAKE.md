---
title: Towards a Machine Learning-Assisted Kernel with LAKE
date: 2023-12-20 08:51:16
tags:
---
# Intro
硬件的演变和多样化正在推动现代操作系统复杂性的爆炸性增长。CPU核心数量已经增长，新的内存技术如HBM和NVM以及像NUMA这样的组织已经变得司空见惯，新的网络和加速技术的出现，所有这些都对操作系统施加了压力，要求其进行高效的资源管理，以保持硬件的承诺。操作系统内核包含了用于管理这些资源的子系统，如内存管理器、I/O和进程调度以及文件系统，目前依赖于启发式方法来处理对性能至关重要的复杂权衡空间。这些启发式方法是通过观察系统行为、结合内核开发者的经验来开发的，目标是提供合理的平均情况性能。

随着硬件和软件复杂性的不断增加，机器学习（ML）已经成为一个有吸引力的替代方案，有潜力更好地导航当前由启发式方法处理的操作系统权衡空间。用ML替换启发式方法可以实现使用实时观察到的行为训练的系统特定解决方案。虽然已经提出了在操作系统子系统中使用ML支持的策略，如CPU负载平衡、文件系统预取、I/O延迟预测、控制CPU时钟和功率等，但以前的工作只关注了ML对单个子系统的潜在益处。我们则关注从将ML决策集成到操作系统内核中产生的系统挑战。

我们研究了五个可以用ML决策增强的基于启发式的内核子系统，包括进程调度、内存管理等。我们发现了一些重要的挑战，包括以下几点。C1 使用专用硬件如GPU/TPU对于降低ML算法的性能影响至关重要，但在内核空间中对加速器的可访问性较差是采用的障碍。加速器卸载在加速器是I/O附加时引入了额外的开销，并且在用户和内核空间使用加速器之间产生了新的争用形式。C2 对于ML的加速的好处是子系统、工作负载和硬件依赖的，因为硬件加速必须摊销数据传输的成本。C3 在抽象层边界和需要跨层数据共享以暴露用于训练和推理的特性之间存在基本的紧张关系。我们在这篇论文中解决了这些挑战，并分享了我们构建学习辅助、加速内核（LAKE）的经验。

为了解决C1，LAKE使用API远程调用为内核空间应用提供供应商支持的加速器接口（例如CUDA API），对于需要使用难以移植到内核空间的库的应用，提供自定义的高级API（例如TensorFlow）。LAKE通过在内核应用和用户空间组件之间进行零拷贝数据移动来减少开销。用户和内核空间对专用硬件的并发使用引入了争用，LAKE使用策略回调框架来管理这种争用。我们发现，管理这种争用所需的相同机制可以被重新用于解决C2，即专用硬件的可变利润性。LAKE提供了一个自定义的策略接口用于争用控制，当策略预测到争用或性能收益不足时，允许内核利用加速器或回退到较少的强度和/或基于CPU的解决方案。LAKE通过一个内核特性存储来解决C3，简化了为收集数据以通知训练和推理的内核子系统进行检测的任务，基于预期异步和抽象层边界等挑战的API。

我们的实验表明，LAKE为内核空间的ML支持的子系统提供了高效的硬件加速，可以减少内核的CPU利用率，并通过争用管理避免对用户空间应用的性能降级。例如，LAKE为ML辅助的I/O延迟预测提供了性能优势，将推理时间减少了高达95%，并将ML驱动的负载平衡推理加速提高了高达3.1倍。由于我们关注的是内核ML集成引起的系统问题，我们依赖于文献中的先前结果，这些结果展示了相对于启发式方法的ML支持策略的改进。然而，我们提出了一个端到端的IO调度案例研究，描述了加速的影响，发现ML的好处得以保留，硬件加速可以启用更丰富的模型。我们发现，LAKE的基础设施也可以用于启用ML领域之外的加速机会。我们评估了GPU加速的文件系统加密，发现潜在的读取吞吐量相对于AES-NI增加了高达62%，CPU利用率降低了高达64%。本文的贡献包括：
• 一个在内核空间暴露以ML为中心的硬件加速的框架（§4），具有管理争用（§4.3）和内核/用户空间硬件加速器共享的可变性能利润性（§4.2）的接口。
• 一个框架和高效的API，简化了在不同内核子系统中的特性收集和管理（§5）。
• 当由LAKE的基础设施驱动时，评估现有内核子系统的CPU利用率降低和性能提升（§7）。

2 背景
2.1 操作系统内核和机器学习
像Linux内核这样的单体内核，随着技术的发展，越来越多地积累了新的特性和责任。例如，Linux最初有一个简单的、贪婪的、时间片调度算法，只有一个任务列表。硬件的演变，例如核心数量的增加、超线程、非均匀内存访问（NUMA）和多个CPU插槽，迫使调度算法演变以支持这些特性。目前，Linux的调度器有一个更复杂的算法，使用自平衡树和每核任务列表，并必须在核心之间进行复杂的负载平衡以保持良好的利用率。这种问题维度的不断增加以及系统可以有不同特性的事实，使得设计高效的、通用的解决方案变得复杂，并导致启发式方法变得僵化，尽管需要解决各种各样的平台。

Linux内核依赖于启发式方法来做出重要的决策，例如回收哪个页面以及如何在CPU之间平衡进程。启发式方法通常是复杂的、计算密集型的、有时不切实际的（例如NP-hard问题）解决方案的便宜替代品。启发式方法的目标是快速得到一个足够好的（局部最小值或最大值）解决方案，而不是花费太多时间探索解决方案空间以寻找最优解。内核使用的启发式方法是一种一刀切的方法，目标是平均情况。例如，一个I/O密集型的服务器和一个计算密集型的服务器，如果使用相同的内核版本，都将使用相同的启发式方法；通过专门针对每个服务器的工作负载进行决策，性能可以得到改善。机器学习是这种固定启发式方法的一个可能的替代方案。

例如，在文件系统预取中，Leap显示，应用程序在文件访问模式上有很高的变化，导致固定的模式查找算法在许多情况下表现不佳。机器学习可以应用于文件系统预取，以改善启发式方法的不足。这可以通过在线学习文件访问模式，在执行过程中，训练自定义模型来实现。

2.2 加速器
专用加速器正在迅速增多：每年都会出现数十种新的特定目的的加速器和框架，以提高计算密集型工作负载的性能和效率。例如，GraphCore IPU和Google TPU等深度学习加速器可以提供比CPU高50倍的能效。近数据计算和分析，例如smartSSDs，将数据平面操作卸载到设备上，因为内部磁盘带宽远高于总线带宽。像GPU这样的通用加速器被广泛用于机器学习、生物信息学、加密货币等。

然而，当前的软件和系统对加速器的支持仅限于用户模式程序。加速器附带用户库和内核驱动，其接口和实现是专有的。尽管存在许多加速器虚拟化技术（例如，固定和中介传递，API远程调用），可以为应用程序提供虚拟GPU，但没有现有的解决方案可以直接被内核空间应用程序使用。

3 动机
我们将ML模型添加到操作系统内核的经验激励我们设计简化集成并赋予开发者使用可能计算密集的算法的基础设施。急需可以被当前和未来的通用应用使用的公共基础设施，以避免特定应用解决方案的增殖。一个关键的挑战是收集推理所需的特征数据，这可能需要在不同的抽象层、不同的模块中查询内核数据结构，这些模块具有不同的锁定规则。我们在第5节提出了一个API设计来应对这个挑战。我们还发现加速器（例如GPU）至关重要。它们的大规模并行性和高吞吐量使得可以实现更复杂和准确的模型；单独使用CPU往往无法满足性能要求。

不幸的是，加速器堆栈通常不会暴露内核空间API，而通常依赖于将专有的高级API支持因子化到用户空间的内核旁路设计。因此，以前的内核加速系统已经使用手工构建的上调用来启用操作系统级别的与加速器的交互。一般的加速器虚拟化技术，如API远程调用是不够的；这些系统使用的通信传输要么不可用，要么对内核和用户空间之间的数据传输效率不高。

将加速器暴露给内核空间揭示了操作系统和ML设置独特的机会和挑战。这个设置独特的关键挑战包括管理内核和用户空间应用程序之间对加速器的争用，减少跨用户-内核边界的不必要的数据移动，并使内核子系统能够根据性能和准确性的利润性在CPU和加速器之间调节。

争用和性能变化。内核ML工作可以与用户空间工作争用访问加速器设备，与跨用户空间进程争用不同，没有明确的机制来管理这种争用。此外，加速必须摊销数据传输成本才能获得性能利润，这需要对输入进行批处理，这可能与内核的延迟目标相冲突。这两种资源管理挑战对操作系统来说都是新的，但操作系统有一个回退的替代方案，即使用CPU。

对性能至关重要的用户应用程序需要稳定、可靠的访问专用硬件，以满足严格的截止期限。内核和用户空间之间对加速器的无节制的争用可能会破坏这些性能和QoS目标。图1演示了当GPU在一个由ML辅助的内核和一个计算绑定的用户进程之间共享时，由争用引起的性能病理现象。用户空间进程正在计算数据哈希，而内核使用GPU来加速页面热度分类和I/O延迟预测。如争用和移动平均线所示，内核和用户空间之间的争用严重影响服务质量。应用程序的吞吐量显著降低和不稳定，降低了高达68%。

数据移动。从内核空间调用用户空间API（通常通过上调用完成）需要对源上下文进行数据编组和复制到用户空间进程，并在完成后复制结果和修改的缓冲区。这可能导致在用户-内核边界上的冗余数据传输和不必要的同步，带来重大的性能损失（§6）。由于没有内核级接口可以将数据传输到加速器，因此必须首先将内核级数据缓冲区复制到用户空间，然后使用如cudaMemcpy等API将其复制到/从加速器。内核机制的智能组合允许自动数据编组和消除用户-内核边界上的数据传输的双缓冲。

3.1 讨论
为什么不直接使用加速器的接口？虽然直接支持内核级加速器API是可能的，但频繁变化的内部接口和缺乏公开可用的文档使得对加速器软件堆栈的部分进行逆向工程变得不切实际。加速器软件堆栈的不透明性要求硬件供应商自己暴露内核级API。尽管NVIDIA最近开源了其驱动程序的一部分[7]，但驱动程序并未向内核暴露必要的高级API。我们还发现，ML支持通常更好地由像TensorFlow这样的高级API提供（§7），因此需要更通用的对上调用的支持。

设备能否直接管理争用？硬件供应商已经表现出他们愿意在硬件中启用一些争用管理。有一些加速器启用了单根I/O虚拟化（SR-IOV），一些设备如SmartNICs和SmartSSDs提供了用于粗粒度争用管理和速率限制的API，或者允许开发者表达他们自己的策略。然而，基于硬件的解决方案往往不够灵活。操作系统内核开发者可能希望动态地在不同的争用管理策略之间进行选择。复杂且不断演变的争用管理策略更容易在软件中表达，而我们的经验是，并非每个加速器都会在硬件中支持细粒度的争用管理策略。机器学习需要额外的策略支持来处理争用管理单独无法解决的性能利润性变化。

隔离是否受到影响？操作系统内核使用地址空间隔离作为他们的主要内存保护机制。我们依赖同样的机制来在将操作系统内核计算卸载到加速器时隔离内存。根据我们的经验，所有的加速器都支持某种类型的地址空间隔离。虽然任何将操作系统内核数据卸载到加速器的方法都可能暴露新的旁路通道，但我们将旁路通道缓解的调查留给未来的工作。

4 使用LAKE进行内核加速
为了允许在内核中使用依赖加速器的复杂机器学习算法，LAKE必须提供基础设施，使得未来和当前的内核空间应用能够使用加速器。目前这是不可能的，因为加速器供应商提供的库是为用户空间设计的。在LAKE中，启用加速器访问内核空间的核心是一个API远程调用系统，该系统向内核子系统公开任意API。LAKE公开的API通过用户空间的一个进程的上调用来执行。图2显示了LAKE的设计。我们考虑一个系统，其中Linux作为主机操作系统，并且至少有一个加速器。尽管这项工作主要关注NVIDIA GPU和CUDA，但没有根本的问题阻止它扩展到其他加速器[85]。

LAKE有三个主要组件：内核侧API提供者（lakeLib）、大量数据内核-用户通信通道（lakeShm）和实现API的用户侧守护进程（lakeD）。lakeLib是一个内核模块，它将加速器的供应商的用户空间库等API公开为内核空间的符号。这个模块有一个与它想要在内核空间支持的API同名的函数。例如，要在内核空间支持cuMemAlloc CUDA API，我们必须在lakeLib中有一个同名的函数。lakeLib中的每一个函数都做三件事：将API标识符和所有API参数序列化成一个命令，通过某个通信通道传输命令以在用户空间远程执行，最后，等待响应。

lakeD是一个用户空间的守护进程，它监听来自lakeLib的命令，对它们进行反序列化并执行请求的API。这个守护进程必须能够访问供应商的库（例如cudart.so）以实现lakeLib请求的API。继续cuMemAlloc API的例子，这样的API的一个命令包括一个字段，该字段标识要执行的API及其参数：要分配多少字节以及一个指针来存储新分配的起始地址。lakeD反序列化命令以获取这些字段，使用供应商的原始库执行API，并通过初始命令来自的同一通道发送回结果：返回码和API调用返回的指针。

最后，lakeShm是一个内核模块，为lakeLib和LAKE驱动的应用提供内存分配。通过lakeShm的API分配的内存被优化用于在内核空间应用和用户空间lakeD之间的数据传输。lakeShm通过请求和映射来自Linux内核的一个大的连续内存区域来工作。当lakeD启动时，同一区域被映射到它的进程。虽然仍然需要主机到设备的传输，但这允许在内核空间模块和lakeD之间进行零拷贝内存移动。

4.1 系统工作流程
当内核空间应用调用LAKE提供的API时，一系列机制被激活，直到最后由加速器处理。这个工作流程包括两个边界交叉：从内核到用户和从用户到内核空间。让我们考虑一个简单的应用，它在本地和GPU上分配内存，将本地数据复制到GPU，并调用一个内核在GPU上进行一些计算。我们研究的所有应用都执行这些步骤。

我们将使用LAKE的应用可以执行的操作分类为三类：本地操作、API远程操作和可复制内存分配。

本地操作：这些操作包括现有的内核函数和内核空间的内存分配。这样的操作不需要远程调用，也不会被LAKE修改。例如，常规的内存分配可以通过调用内核的内存分配器（例如vmalloc）来满足。

API远程操作：LAKE通过lakeLib将加速器API提供给内核空间。当应用调用一个加速器API时，执行流程切换到lakeLib模块。创建一个足够大的命令缓冲区来容纳API函数标识符（例如，一个数字）和所有函数参数。然后，这个命令通过一个类似于套接字的通道发送给lakeD。一旦在用户空间，命令被反序列化，请求的API在加速器上执行。完成后，构建一个带有返回值的返回命令并发送回来。执行API时产生的错误被转发给应用，应用必须进行自己的错误检查。

可复制内存分配：应用程序使用的将被复制到/从加速器的内存区域，应该使用lakeShm分配，它提供了一个类似于malloc的函数。通过lakeShm分配的内存区域是共享的，避免了内核和用户空间之间的内存复制。使用lakeShm本身并不能产生内核空间应用和加速器之间的零拷贝数据传输。例如，CUDA API有一个用户空间API（cudaMallocHost），它提供了从用户空间到GPU的零拷贝传输，但LAKE不能集成这个特性，因为CUDA运行时库是闭源的。对于LAKE提供的自定义高级API（在§4.4中讨论），其中内核空间应用调用的API比在加速器上分配内存的级别要高，lakeShm移除了其领域下唯一的数据复制。如果应用程序不使用lakeShm也不使用减少数据传输的加速器特定API，API远程调用仍然会工作；这只会导致额外的数据复制。

4.2 调节加速器使用
如我们在第7节中所示，使用加速器的盈利性并不总是有保证；加速器的大规模并行性只有在处理大量数据时才有优势。加速器在ML训练中几乎无处不在，因为它的批处理，但对于推理来说并非如此。对小批量输入的推理通常在CPU上更快。通常存在一个批量大小，在这个大小下，加速器会产生更好的性能（我们称之为交叉点）。

为了给内核ML应用提供最佳性能，LAKE允许在CPU和加速器之间进行即时切换，粒度为函数调用。这是通过自定义执行策略完成的（在第4.3节给出了一个例子）。LAKE允许开发者使用eBPF[4]编写和安装这样的策略。通过回调，开发者可以指定考虑使用加速器盈利的必要条件。策略在应用的执行过程中由内核自动执行。图3显示了一个简单的CUDA设备策略的伪代码，该策略通过在批量大小低于某个阈值时回退到CPU来管理可变的盈利性。

4.3 争用管理
我们不能假设LAKE提供的加速器将仅供内核使用。用户空间应用期望从加速器获得性能保证，我们不能容忍性能干扰。当加速器成为一个争用的资源时，内核空间应用必须减少或完全停止使用加速器，并回退到一个更简单、强度较小的加速器实现或CPU实现。

用于调节加速器利用率的相同策略可以用于管理争用。策略的工具集包括任何操作系统或供应商提供的实用程序（例如，由LAKE支持的NVIDIA的NVML API），允许对系统当前状态的细粒度信息。图3显示了一个简单的CUDA设备争用策略的伪代码。该策略对GPU利用率的查询进行速率限制，并使用移动平均数来保持内核对GPU计算的消耗在一个阈值以下。开发者可以用两个回调函数来指定策略：dev_func回调通常包含一个或多个cuLaunchKernel调用，而cpu_func可以包含执行相同计算的替代API，但可能在CPU上操作或使用较少的加速器资源。