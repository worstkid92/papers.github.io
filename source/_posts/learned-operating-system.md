---
title: learned operating system
date: 2023-12-06 10:12:31
tags:
---
# Abstract
操作系统是计算机系统的核心，数十年来，人们一直致力于操作系统的研究和工程开发。为了跟上现代硬件和应用的发展速度，我们认为在未来的操作系统开发中应采取不同的方法。我们不应完全依赖人类的智慧，而应利用人工智能和机器学习技术来自动 "学习 "如何构建和调整操作系统。本文探讨了 "学习 "操作系统方法的机遇和挑战，并就如何构建这样的操作系统向未来的研究人员和从业人员提出了建议。

# 1 Introduction
在所有类型的软件中，操作系统可能是最复杂、最错综复杂的类型。操作系统几乎是所有计算机系统的核心。它们管理硬件资源，为应用程序的执行提供受保护的环境。因此，操作系统的设计会影响在其上运行的所有应用程序。传统上，操作系统都是由专家通过长期、反复的工程设计构建而成。大多数操作系统（如 Linux 和 Windows）都采用通用设计，并在安装时或安装后留有各种调整选项。通常的做法是以默认配置安装操作系统，并在需要时更改特定配置。 这种构建和调整操作系统的长期做法有四个局限性。首先，操作系统发展缓慢，改变一个成熟的操作系统很难。然而，如今的硬件和应用程序变化很快。虽然新的硬件和应用可能不需要从头开始构建全新的操作系统，但重写或添加某些操作系统功能在很大程度上会使它们受益。遗憾的是，传统的手动操作系统开发方法无法跟上硬件和应用不断发展的步伐
其次，很难正确调整操作系统。现代操作系统有许多会影响应用程序性能的配置。例如，Linux v5.1（最新版本）的内核配置总数超过 17K。手工调整大量操作系统配置的过程既漫长又特别。此外，这样做也无法达到最佳效果。 第三，操作系统不会在运行时 "改变"。在构建、安装和配置操作系统后，其功能、策略和参数都保持不变。因此，当今的操作系统无法动态适应应用程序不断变化的行为和需求。
最后，通用操作系统无法很好地支持各类应用程序或硬件。当今主流操作系统的大多数功能都是为通用目的而设计的。 可配置参数仅允许对某些操作系统功能进行有限的专业化。例如，Linux 交换系统提供了一个名为 swappiness 的参数，用于控制交换内存页的频率，但它在选择交换页时总是使用 LRU 策略。此外，swappiness 是一个全局参数，在 Linux 操作系统上运行的所有应用程序都必须使用相同的 swappiness 值。随着未来硬件和应用程序的异构性越来越强，操作系统应允许更多的特殊化.
我们认为，这些局限性要求我们重新思考传统的操作系统架构。我们的答案是利用机器学习（ML）技术来构建和配置操作系统，我们称这种方法为 "学习型 "操作系统。例如，我们可以利用 ML 预测操作系统的最佳配置，这样的配置可以在运行时不断适应应用程序的变化。 人工智能还可用于生成某些操作系统功能的策略和机制。通过设计和构建框架，为操作系统训练和使用 ML 模型，我们可以避免操作系统开发过程中的大量工程工作。此外，基于 ML 的方法有可能产生更好的结果，对不同的应用具有更强的适应性和微调能力。 然而，"学习 "操作系统并非易事。之前有一些使用 ML 技术生成或改进（简单）操作系统策略的尝试，但都没有被大规模采用。尽管如此，随着最近将人工智能技术成功应用于数据库等底层软件，我们相信，学习型操作系统不仅是可行的，而且比目前利用启发式方法和人类经验构建的操作系统更高效。

# 2 Opportunities for ML in OSes
机器学习可以辅助或取代至少三种类型的传统操作系统组件。首先，机器学习可用于（动态）在操作系统中设置许多配置。其次，机器学习技术可用于根据应用程序行为和硬件属性在操作系统中生成策略。最后也是最积极的方法是使用机器学习来构建某些操作系统机制。
## 2.1 学习配置
现代操作系统包含数千种配置可以由特权用户在操作系统安装时或之后进行设置。例如，Linux-5.1 中的内存系统（“mm”）、文件系统（“fs”）和网络系统（“net”）有 89、351 和 729 种配置。 在所有类型的 Linux 中配置，至少有两大类可以直接影响应用程序的性能，并且可以在很大度上从机器学习方法中受益。首先，Linux中有很多与时序相关的配置，例如中断CPU核心的频率（对于线程调度），调用后台交换的频率（用于内存分页），刷新缓冲区缓存的频率（用于存储），以及CPU时钟频率的采样率（用于能源和性能）。 设置这些与时序相关的配置很困难，因为需要进行各种权衡。 例如CPU频繁中断提供了提高 CPU 利用率的机会（更多积极的线程调度），但可能会导致性能开销（通过抢占和上下文切换线程），这反过来，又会降低 CPU 的有效利用率。
其次，各种类型的配置有很多大小，例如缓冲区缓存大小（用于存储缓存）、磁盘预取量（用于存储访问）和交换预取数量（用于内存分页）。 设置这些与大小相关的配置很困难，特别是当需要权衡时不同的尺寸。 例如，更大的缓冲区高速缓存可以改善存储系统的性能，但降低了可用用户应用程序的内存。
上述两种类型的配置中的许多都会极大地影响应用程序性能和其他重要的配置能源成本等指标。 然而，设置它们的做法长期以来一直是一项涉及繁重的工程和人力的工作：通过启发式、反复试验或离线实验。 而且，一旦设定，就很少改变。
ML 更适合设置这些操作系统配置。使用过去的工作负载和操作系统/硬件环境训练的良好机器学习模型可能会优于人类设置的配置。该模型可以继续动态生成新的配置以适应工作负载和环境的变化。强化学习是一种很有前途的生成操作系统配置的机器学习技术。尽管强化学习的推理过程成本更高，但它很适合我们的需求，因为操作系统配置只需要很少重新生成。
## 2.2 学习策略
操作系统需要做出许多决定。这些决策通常会影响应用程序性能和资源利用率，但很少影响正确性。可以动态适应不同应用程序行为的机器学习技术有可能超越当前操作系统基于启发式的全局静态策略。下面，我们讨论可以通过机器学习生成的几种类型的操作系统策略。
空间分配。 操作系统管理硬件资源的一个关键任务是空间分配。 当应用程序请求内存或存储空间时，操作系统需要决定为应用程序提供哪些可用空间。 很多这些分配政策基于启发式和简单的算法。 例如Linux分配虚拟内存空间对于使用最适合策略的 mmap 系统调用（即，选择适合所请求的 mmap 的最小虚拟地址孔尺寸）。 Linux ext 系列文件系统分配近邻同一目录下的文件的空格。 尽管这些策略适用于许多工作负载和用途，但它们并不是唯一的选择。所有类型应用的最佳选择。 例如，文件同一目录下的文件很可能被一起访问，将它们放置得彼此靠近可以节省磁盘寻道。但是，当用户访问不同目录下的文件时，当前 ext 文件系统的分配策略不起作用。
为了更好地做出空间分配决策，操作系统可以使用用于预测分配候选位置的 ML 模型。构建良好的机器学习模型对于基于机器学习的空间分配的成功至关重要。 一个可行的方法是开始构建通过分析用户请求多少空间、什么空间的历史痕迹来建立模型（全局、每个用户或每个应用程序）
分配的操作系统、如何有效利用空间（即如何有很多碎片），以及用户如何访问分配的空间。 然而，静态模型还不够，因为应用程序行为可能会发生变化，并且可能会出现新类型的应用程序。 我们期待一些在线学习技巧将需要不断更新空间分配机器学习模型。
调度。 操作系统会做出多种类型的调度决策。 对于 CPU 调度，操作系统决定运行哪些线程在每个CPU核心上。 当一个正在运行的进程的时间片过期或进程放弃其运行核心时，Linux（自 v2.6.23 起）使用 CFS（完全公平调度）默认情况下，策略决定接下来运行哪个线程。 操作系统还
管理各种队列，例如网络队列和存储队列。他们在这些队列上安排请求/操作，以获得更好的性能、公平性和负载平衡。 设置好调度策略是困难的，好策略的标准可以不时改变。 例如，在 CFS 之前，较旧的Linux 中的版本使用 O(1) CPU 调度程序。不是手动调整调度策略，而是动态调整
使用机器学习生成的调度决策可以极大地提高操作系统的效率。 另外，与传统调度相比政策、机器学习模型有可能运行得更快并节省成本元数据存储空间。 例如Linux CFS调度器使用红黑树来存储可用的虚拟内存地址范围，需要 O(logN) 时间才能做出决定。
缓存管理。 缓存是一种广泛使用的技术操作系统。 操作系统虚拟内存系统将热数据存储在物理内存中内存并将其余部分保留在速度较慢的存储设备中。 文件系统使用内存缓冲区高速缓存来存储热文件数据和元数据。 管理缓存时，操作系统需要决定何时以及要驱逐哪些数据。 目前，操作系统使用一组固定的缓存驱逐策略，通常是多年的研究和工程工作结果. 例如，大多数操作系统虚拟内存系统尝试交换出以下内存页最近最少使用某种形式的近似 LRU。 这样的策略对于显示良好时间性的工作负载效果很好地方，但不适合那些地方不好或那些其局部性无法被近似 LRU 捕获
政策。
操作系统可以使用 ML 来代替像 LRU 这样的固定策略决定缓存驱逐的候选者。 这样的机器学习模型可以是一起学习使用过去的内存/存储访问模式与学习缓存大小（第 2.1 节）。 我们将讨论学习第 3.1 节中相关目标的挑战。
## 2.3 学习机制
大多数配置和策略并不影响操作系统的正确性，因此机器学习是它们的良好候选者。更具挑战性的任务是使用机器学习进行需要精确的操作系统任务。在操作系统中有许多这样的任务，它们通常是实现某些功能的机制。
受到学习索引的启发，我们确定了两种可以用机器学习模型替换的操作系统中的“机制”，它们都执行将一种抽象映射到另一种抽象的功能。第一个是从虚拟内存地址映射到物理内存地址，目前由页表执行。第二个是从文件名和偏移量映射到磁盘逻辑块地址，目前由文件系统多级索引结构完成。这两种类型的“映射表”对所有内存和存储系统的性能至关重要，已经投入了大量的研究和工程努力来改进它们。
内存和文件映射都可以从机器学习方法中受益。机器学习模型有可能降低内存和文件映射的性能和空间成本。此外，机器学习模型是灵活的，可以定制到任何类型的工作负载。与今天的内存系统中的固定大小的内存页不同，基于机器学习的映射可以推断任何大小和偏移的内存空间。此外，机器学习模型可能比多级页表更小，运行速度更快。我们可以通过将模型参数存储在连续的内存空间中来进一步提高其性能，以提高空间局部性和CPU缓存命中率。
# 3挑战和潜在解决方方案
## 3.1 Model Building
第二部分我们分析了操作系统的哪一部分可以从机器学习中受益，即什么可以被学习。接下来的步骤是设计一个学习方法。在许多机器学习应用中，机器学习模型选择是一个困难的问题。虽然像AutoML这样的系统可以在很大程度上自动化机器学习模型的选择，但它们只适用于像图像识别和自然语言处理这样的研究得比较透彻的问题。在为操作系统选择合适的模型时，有许多独特的挑战。
首先，我们如何判断一个模型的好坏呢？一个看似直接的方法是评估在应用了一个模型后，应用程序的性能变化。然而，应用程序的性能可能会受到许多因素的影响，比如工作负载的变化，同一操作系统上运行的其他工作负载，以及操作系统的其他部分。为了更好地确定某个模型的效果，我们应该寻求更好、更局部化的评估目标。例如，我们可以使用缓冲区缓存未命中率来确定预测缓冲区缓存替换策略的模型的好处，而不是应用程序的性能。
接下来，我们应该构建全局的机器学习模型，每个用户的模型，还是每个应用程序的模型呢？更细粒度的模型可以通过更多的定制和专业化实现更准确的预测，但需要更多的资源（例如，内存空间，用于训练的CPU时间）。
在预测过程中的另一个潜在优化是生成多个候选项或预测未来的多个步骤。例如，分页驱逐模型可以返回前K个驱逐的候选项，操作系统只需要在K个页面驱逐中进行一次推理。这样做可以减少在每一步生成单个候选项的性能开销。
最后一个主要的挑战是在操作系统中学习多个相关的任务。不同的配置、策略和机制都可以影响操作系统子系统的性能，有时它们甚至可以相互关联。例如，缓冲区缓存的大小、刷新频率和驱逐策略都可以影响缓冲区缓存的性能（从而影响存储系统的性能）。缓冲区缓存的大小也可以影响内存系统的性能。因此，我们应该联合优化相关任务以获得最佳结果。我们设想在这种情况下，多任务学习会很有帮助。
## 3.2 training
对于操作系统来说，训练机器学习模型面临着独特的挑战。
首先，收集训练数据应该对前台应用程序造成最小的开销。例如，追踪每次内存访问以构建预测内存驱逐候选项的模型是不可行的。大量的训练数据也会导致过高的空间开销。另一方面，没有足够的训练数据可能会降低机器学习模型的准确性。一种可能的解决方案是使用预先收集的、代表性好的训练数据进行离线训练，然后使用在线训练（不频繁地）更新构建的模型。离线训练可以使用更细粒度的数据，而在线训练，我们可能只能收集粗粒度的数据，以免干扰前台应用程序的性能。
其次，我们如何构建验证集？对于某些问题，存在基本事实或理论最优解。例如，对于CPU调度，理论上最好先运行剩余时间最短的作业（以获得最快的周转时间）；对于页面替换，理论上最好的是驱逐最远未来不会被使用的页面。这些理论上最好的解决方案可以直接用作训练期间的验证集。然而，对于其他问题，没有明确的最佳解决方案。一种可能的方法是让用户通过提供奖励函数来定义他们的应用程序的需求或目标，而不是提供一个验证集。操作系统可以使用强化学习技术来找到满足这些需求/目标的最佳解决方案。
## 3.2 推理
不同的操作系统学习目标对推理速度有不同的标准。第一类学习只需要偶尔运行一次或在工作负载变化时运行，即前台应用程序操作不需要等待这些推理结果。配置和策略都属于这一类别。对于这种类型，推理可以运行得稍微慢一些，这样我们就可以探索更昂贵的机器学习技术，如强化学习。
操作系统的第二类“决策”必须非常快，因为它们在应用程序性能关键路径上。例如，操作系统需要在线程开始执行之前决定在一个核心上调度哪个线程，这意味着决策的紧密界限应该在上下文切换的时间内或者最多在上下文切换的时间附近。有了快速的存储和网络设备，操作系统文件/存储和网络系统中的决策也需要快速做出（大约或在1微秒内）。GPU和其他专用处理器（如TPU）可以使用复杂的模型进行快速预测，但是调用它们仍然需要很长时间（使用今天的PCIe需要1-2微秒）。为了能够为这些操作系统用途快速地进行推理，我们要么需要减少这种调用成本，要么需要减少模型复杂性并在本地CPU上运行。
除了执行推理的性能开销外，还有存储用于推理的机器学习模型的内存空间开销。大型模型可以轻易地占用数百MB的内存。当操作系统中有数百到数千个配置、策略和机制需要学习时，模型空间成本可能会过高。一种有前景的方法是利用循环神经网络中的模型内存重用技术来减少内存消耗。
## 3.4 在操作系统中集成机器学习
单独的机器学习不能构成一个完整的操作系统。我们预见到在现有的操作系统中集成机器学习模型及其预测会有几个挑战。
首先，虽然大多数操作系统策略只用于性能提升，可能是次优的或者“错误的”，但是一些操作系统功能，如文件系统和虚拟内存映射，需要精确。如何使用本质上是概率性的机器学习技术来实现操作系统的确定性任务，是一个有趣但具有挑战性的问题。一种可行的方法是让机器学习模型首先做出一系列的概率预测，然后使用传统算法在这个范围内计算最终的精确答案。机器学习预测的范围越精确，精确搜索的第二步就越快。
除了正确性保证，如何在内核空间运行机器学习模型也是一个新的挑战。与用户空间不同，内核空间缺乏机器学习库的支持。为了让内核使用机器学习技术，可能的选择在于构建新的内核空间机器学习库和在用户空间运行机器学习，然后将结果传回到内核空间。第一种选择需要大量的工程工作，而后一种选择可能会因为上下文切换而导致性能损失。
最后，像Linux这样的现代操作系统采取了单一内核的方法，并且在几十年的开发工作中变得非常复杂。在现有的操作系统中集成机器学习将需要精心的工程设计，以最小化对操作系统其余部分的干扰。
## 3.5 安全性
操作系统应该为应用程序提供对硬件资源的保护访问。使用学习的操作系统方法，使用应用程序数据学习的机器学习模型在操作系统中使用是否安全呢？尽管我们设想所有与安全和保护相关的任务在操作系统中仍然以传统的方式实现，但是在操作系统内核的其余部分使用机器学习可以引入独特的安全威胁和影响。
当在操作系统中应用机器学习技术时，用户数据将间接地参与到操作系统的控制平面中。如果没有适当的保护，恶意用户将能够通过提供精心制作的数据来操纵训练和推理过程。这样做可能会导致操作系统使用错误的机器学习模型，这些模型对攻击者有利。例如，攻击者可以训练一个机器学习模型，使其总是驱逐其他应用程序的内存，并发起拒绝服务攻击。它还可以生成一个糟糕的机器学习模型，导致操作系统不断地预测失误，性能下降。对不同的应用程序使用不同的机器学习模型（即机器学习的隔离）可以大大提高学习操作系统的安全性。然而，保护免受侧信道攻击和防止信息泄露仍然是一项挑战。
# 4 相关工作
针对其他低级系统的机器学习。最近的学习索引工作提出了用预测数据位置的神经网络模型替换传统的基于树的索引结构。学习索引启发了许多后续的研究工作。事实上，这篇论文也受到了它的启发。学习索引的后续工作是SageDB，一个学习数据库系统。除了索引，它还使用机器学习进行合并和哈希连接操作。学习索引的另一个扩展是学习布隆过滤器。还有许多提议使用机器学习来解决各种硬件问题。更相关的是一项最近的工作，该工作使用循环神经网络（RNN）预测内存访问模式并执行内存预取。
针对操作系统的机器学习。尽管机器学习已经在许多领域中使用，最近在低级系统中的使用也更多，但操作系统很少采用任何机器学习技术，大多数研究提议都可以追溯到几十年前。例如，有几个提议使用机器学习技术（例如C4.5决策树，线性回归）来改善应用程序作业的平均周转时间，例如，通过调整内核抢占时间和通过预测作业运行时间。Lynx是一个使用机器学习更好地执行SSD预取的系统。它利用马尔可夫链来检测I/O工作负载模式并计算文件页之间的转移概率。
# 5 结论
机器学习技术的进步以及“大数据”和计算资源的可用性使得我们可以在许多以前依赖人力的领域应用机器学习。我们相信操作系统也是这样一个领域。本文系统地探讨了在操作系统中使用机器学习的机会和挑战。这当然只是构建真正的学习操作系统解决方案的起点。我们预期会出现更多的设计、开发和部署挑战。然而，我们相信学习操作系统是一个值得探索的方向，希望本文能激发和帮助未来在这个领域的研究者和从业者。

# 6 读后感
 Cited：Toward reconfigurable kernel datapaths with learned optimizations HOS21 针对policy，做in-kernel的ML
 Operating Systems for Resource-adaptive Intelligent Software: Challenges and Opportunities

 policy是成本/后果最小的
 configuration的学习成本很高
 mechenism就文中的例子来说收益可能是最高的，但是

